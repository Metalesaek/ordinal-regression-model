---
title: "ordinal regression model"
author: "Dr.metales"
date: "12/25/2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,warning = FALSE,error = FALSE,message = FALSE)
```


## Introduction

In this paper we will fit several models to the **heart disease** data [uploaded from kaggle website](https://www.kaggle.com/johnsmith88/heart-disease-dataset).
You can also download this data directly from the html output of this rmarkdown file to nay format you want (at least the formats listed above the table).

## Data praparation

First, we call the data and the libraries that we need along this illustration as follows.


```{r}
library(tidyverse)
library(caret)
mydata<-read.csv("heart.csv",header = TRUE)
names(mydata)[1]<-"age"
```

The data at hand has the following features:

* age.
* sex: 1=male,0=female
* cp : chest pain type.
* trestbps :  resting blood pressure.
* chol: serum cholestoral.
* fbs : fasting blood sugar.
* restecg : resting electrocardiographic results.
* thalach : maximum heart rate achieved
* exang : exercise induced angina. 
* oldpeak : ST depression induced by exercise relative to rest.
* slope : the slope of the peak exercise ST segment.
* ca : number of major vessels colored by flourosopy.
* thal : it is not well defined from the data source.
* target: have heart disease or not.


```{r}
creat_dt <- function(x){
  DT::datatable(x,
                extensions = "Buttons",
                options = list(dom="Blfrtip",
                buttons=c("copy","csv","excel","pdf","print"),
                lengthMenu=list(c(10,25,50,-1),
                                c(10,25,50,"All")
                  
                )
                ))
}

creat_dt(mydata)
```


For our case we will use the chest pain type **cp** variable as our target variable since it is a categorical variable. However, for pedagogique purposes, we will manipulate it so that it will be an ordered factor with only three levels **no pain**,**moderate pain**, **severe pain** (instead of 4 levels now).   

First, we convert the variables that chould be of factor type, next we drop the the less frequently level from the **cp** variable and all the related rows, then we rename its lavels as **no** pain for the most frequently one, **severe** pain for the less frequently one, and **moderate** pain for the last one.   

Now let's include all the predictors except those factors that do not satisfy the threshold of 5 cases in the cross tables, and we remove the **cp level** "3" as we did earlier.

```{r}
mydata<-mydata %>%
  modify_at(c(2,3,6,7,9,11,12,13,14),as.factor)
mydata<-mydata[mydata$cp!=3,]
mydata$cp<-fct_drop(mydata$cp,only=levels(mydata$cp))
table(mydata$cp)
```

According to these frequencies we rename and we order the levels as follows.

```{r}
mydata$cp<-fct_recode(mydata$cp,no="0",sev="1",mod="2")
mydata$cp<-factor(mydata$cp,ordered = TRUE)
mydata$cp<-fct_infreq(mydata$cp)
mydata$cp[1:5]
```

Similar to the logistic regression, the number of cases in each cell from each cross table between the outcome and each factor should be above the threshold of 5 applied in practice.


```{r}

xtabs(~cp+sex,data=mydata)
xtabs(~cp+target,data=mydata)
xtabs(~cp+fbs,data=mydata)
xtabs(~cp+restecg,data=mydata)
xtabs(~cp+exang,data=mydata)
xtabs(~cp+slope,data=mydata)
xtabs(~cp+ca,data=mydata)
xtabs(~cp+thal,data=mydata)
```

The following variables do not respect this threshold nad hence they will be removed from the predictors set: **restecg**, **exang**, **slope**, **ca**, and **thal**.

```{r}
mydata<-mydata[,-c(7,9,11,12,13)]
```

Now let's partition the data and train the model.


```{r}

set.seed(1122)
index<-createDataPartition(mydata$cp,p=.8,list=FALSE)
train<-mydata[index,]
test<-mydata[-index,]
```


for this data we will use ( different model).


## ordered logistic regression:


Before training this type of model let's show how it works. For simplicity suppose we have data that has an ordered outcome $y$ with three classe labels ("1","2","3") and only two features $x_1$ and $x_2$.  

First we define a latent variable as a linear combination of the features:

$$y_i^*=\beta_1 X_{i1}+\beta_2 X_{i2}$$

Then since we have three classes we define two thresholds for this latent variable $\alpha_1$ and $\alpha_2$ , and a particular observation will be classified $y_i$ will be classified as follows:


\[\begin{cases} y_i=1 & \text{if $y_i^* \leq \alpha_1$} \\
                y_i=2 & \text{if $\alpha_1 < y_i^* \leq \alpha_2$} \\
                y_i=3 & \text{if $y_i^* > \alpha_2$}\end{cases}\]


Now we can obtain the probability of a particular observation to fall into a specific class as follows:


\[\begin{cases} p(y_i=1)=p(y_i^* \leq \alpha_1)=F(\alpha_1-\beta_1 X_{i1}-\beta_2 X_{i2}) \\
                p(y_i=2)=p(\alpha_1 < y_i^* \leq \alpha_2)=F(\alpha_2-\beta_1 X_{i1}-\beta_2 X_{i2})-F(\alpha_1-\beta_1 X_{i1}-\beta_2 X_{i2}) \\
                p(y_i=3)=1-p(y_i=2)-p(y_i=1)\end{cases}\]


It reamins now to define the suitable distribution function F. There are two  commonly used ones for this type of data, the **logit** function  $F(x)=\frac{1}{1+exp^{-x}}$ and the the normal distribution function  aka **probit**.

Using the **logit** function the probabilities will be. 


\[\begin{cases} p(y_i=1)=\frac{1}{1+exp^{-(\alpha_1-\beta_1 X_{i1}-\beta_2 X_{i2})}} \\
                p(y_i=2)=\frac{1}{1+exp^{-(\alpha_2-\beta_1 X_{i1}-\beta_2 X_{i2})}}-p(y_i=1) \\
                p(y_i=3)=1-p(y_i=2)-p(y_i=1)\end{cases}\]


```{r}
library(MASS)
set.seed(1234)
model_logistic<-train(cp~., data=train,
                      method="polr",
                      tuneGrid=expand.grid(method="logistic"))
```

```{r}
summary(model_logistic)
```

To get the significance we can and the p_values as follows.


```{r}
prob <- pnorm(abs(summary(model_logistic)$coefficients[,3]),lower.tail = FALSE)*2
cbind(summary(model_logistic)$coefficients,prob)
```

Using the p values we will remove the non significant variables. **age**, **trestbps**, **chol**.


```{r}
set.seed(1234)
model_logistic<-train(cp~.-age-trestbps-chol, data=train,
                      method="polr",tuneGrid=expand.grid(method="logistic"))
prob <- pnorm(abs(summary(model_logistic)$coefficients[,3]),lower.tail = FALSE)*2
cbind(summary(model_logistic)$coefficients,prob)

```

Notice that we do not remove the factors **sex** and **fbs** even they are not significant due to the significance of the intercepts.

To well explain these coefficients lets restrict the model with only two predictors.


```{r}

set.seed(1234)
model1<-train(cp~target+thalach, 
              data=train,
              method = "polr",
              tuneGrid=expand.grid(method="logistic"))
summary(model1)
```



Let's plug in these coefficients in the above equations we obtain the probability of each class as follows:



\[\begin{cases} p(no)=\frac{1}{1+exp^{-(5.1590-1.82179X_{i1}-0.02717X_{i2})}} \\
                p(mod)=\frac{1}{1+exp^{-(7.0698 -1.82179X_{i1}-0.02717X_{i2})}}-p(no) \\
                p(sev)=1-p(mod)-p(no)\end{cases}\]




Let's predict a particular patient.


```{r}
train[3,c("cp","thalach","target")]

```



\[\begin{cases} p(no)=\frac{1}{1+exp^{-(5.1590-1.82179X_{i1}-0.02717X_{i2})}} \\
                p(mod)=\frac{1}{1+exp^{-(7.0698-1.82179X_{i1}-0.02717X_{i2})}}-p(no) \\
                p(sev)=1-p(mod)-p(no)\end{cases}\]



```{r}
1/(1+exp(-(5.1590-1.82179*1-0.02717*163)))
1/(1+exp(-(7.0698-1.82179*1-0.02717*163)))

```



\[\begin{cases} p(no)=0.2513359 \\
                p(mod)=0.6940877-0.2513359=0.4427518 \\
                p(sev)=1-0.4427518-0.2513359=0.3059123\end{cases}\]


Using the highest probability this patient will be predicted to have **no** pain.
Now let's compare these probabilities with those obtained from function **predict**

```{r}
predict(model1,train[1:3,], type="prob")

```



Now lets get the confusion matrix for the training set.


```{r}
pred<-predict(model_logistic,newdata = train)
confusionMatrix(pred,train$cp)
```

Th eaccuracy rate of the training set is about 62%.

For the test set now.



```{r}
pred<-predict(model_logistic,newdata = test)
confusionMatrix(pred,test$cp)
```

The accuracy rate is about 61.82%

## CART model

This is tree-based model. To train this model we make use of **rpartScore** package, and for simplification we will include only the significant predictors from the previous model. 



```{r}
library(rpartScore)
set.seed(1234)
model_cart<-train(cp~.-age-trestbps-chol, data=train,
                      method="rpartScore")

```

```{r}
model_cart
```

the largest accuracy rate is about 59.59%, with the **split** equals **abs** and **prune** argument equals **mc**.   
The argument **split** controls the spplitting function used to grow the tree by setting the msiclassification costs in the generalized **Gini** impurity function to the absolute **abs** or squared **quad**. 
The argument **prune** to select the performance measure to prune the tree between total misclassification rate **mr** or misclassification cost **mc**.

Let's get the confusion matrix.


```{r}
pred<-predict(model_cart,newdata = test)
confusionMatrix(pred,test$cp)
```

We can see that the accuracy rate of this model is larger than the previuos one 63.64%.


## Random forst model.

To train ordinal random we need to call the following packages:
**e1071**, **ranger**, **ordinalForest**.  



```{r}
library(ordinalForest)
library(ranger)
library(e1071)
set.seed(1234)
model_forest<-train(cp~.-age-trestbps-chol, data=train,
                      method="ordinalRF")

```


```{r}
model_forest
```


```{r}
plot(varImp(model_forest))

```




```{r}
pred<-predict(model_forest,newdata = test)
confusionMatrix(pred,test$cp)
```

with this model the accuracy rate is 61.82%

## Cumulative probability model

For this model we need the **VGAM** package.



```{r,message=FALSE,error=FALSE,warning=FALSE}
library(VGAM)
set.seed(1234)
model_vgam<-train(cp~.-age-trestbps-chol,
                                            data=train,
                                            method="vglmCumulative",
                                            trace=FALSE)

```

```{r}
model_vgam
```

the best model is obtaind when the argument **parallel** is FALSE and **link** is **cauchit** which is the tangent function .

```{r}
plot(model_vgam)
```


```{r}
model_vgam$finalModel
```




```{r}
pred<-predict(model_vgam,newdata = test)
confusionMatrix(pred,test$cp)
```

We have now a good accuracy rate which is about 67.27%


## Compare models

We can conmpare between the above models using **resample** caret function.

```{r}
models_eval<-resamples(list(logit=model_logistic,
                            caret=model_cart,
                            forest=model_forest,
                            vgam=model_vgam))
summary(models_eval)
```

 Using the mean of the accuracy rate we can say that **vgam** model is the best model for these data with 59.73% for the training set and with 67.27% for the test set as we have obtained.
 
we can plot these model.

```{r}
bwplot(models_eval)
```


